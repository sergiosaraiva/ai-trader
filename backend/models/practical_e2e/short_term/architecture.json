{
  "input_dim": 42,
  "sequence_length": 10,
  "output_dim": 1,
  "hidden_dim": 24,
  "num_layers": 1,
  "dropout": 0.3,
  "use_batch_norm": true,
  "activation": "relu",
  "output_type": "regression",
  "num_classes": 3,
  "prediction_horizons": [
    1
  ],
  "architecture_name": "cnn_lstm_attention"
}